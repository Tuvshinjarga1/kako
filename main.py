import os
import time
import logging
import requests
import json
from urllib.parse import urljoin, urlparse
from flask import Flask, request, jsonify, render_template_string, send_from_directory
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, Optional

# Groq AI client –∏–º–ø–æ—Ä—Ç
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    logging.warning("Groq client not installed. Install with: pip install groq")

app = Flask(__name__, static_folder='.', static_url_path='/static')
logging.basicConfig(level=logging.INFO)

# ‚Äî‚Äî Config ‚Äî‚Äî #
ROOT_URL             = os.getenv("ROOT_URL", "https://kako.mn/")
DELAY_SEC            = float(os.getenv("DELAY_SEC", "0.5"))
ALLOWED_NETLOC       = urlparse(ROOT_URL).netloc
MAX_CRAWL_PAGES      = int(os.getenv("MAX_CRAWL_PAGES", "500"))
CHATWOOT_API_KEY     = os.getenv("CHATWOOT_API_KEY")
ACCOUNT_ID           = os.getenv("ACCOUNT_ID")
CHATWOOT_BASE_URL    = os.getenv("CHATWOOT_BASE_URL", "https://app.chatwoot.com/")
GROQ_API_KEY         = os.getenv("GROQ_API_KEY")
AUTO_CRAWL_ON_START  = os.getenv("AUTO_CRAWL_ON_START", "true").lower() == "true"

# Initialize Groq client
client = Groq(api_key=GROQ_API_KEY) if (GROQ_API_KEY and GROQ_AVAILABLE) else None

# ‚Äî‚Äî Memory Storage ‚Äî‚Äî #
conversation_memory = {}
crawled_data = []
crawl_status = {"status": "not_started", "message": "Crawling has not started yet"}

# ‚Äî‚Äî Crawl & Scrape ‚Äî‚Äî #
def crawl_and_scrape(start_url: str):
    visited = set()
    to_visit = {start_url}
    results = []

    while to_visit and len(visited) < MAX_CRAWL_PAGES:
        url = to_visit.pop()
        if url in visited:
            continue
        visited.add(url)

        try:
            logging.info(f"[Crawling] {url}")
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            logging.warning(f"Failed to fetch {url}: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.string.strip() if soup.title and soup.title.string else url
        body, images = extract_content(soup, url)

        results.append({
            "url": url,
            "title": title,
            "body": body,
            "images": images
        })

        for a in soup.find_all("a", href=True):
            href = a["href"]
            if is_internal_link(href):
                full = normalize_url(url, href)
                if full.startswith(ROOT_URL) and full not in visited:
                    to_visit.add(full)

        time.sleep(DELAY_SEC)

    return results

# ‚Äî‚Äî Startup Functions ‚Äî‚Äî #
def auto_crawl_on_startup():
    """Automatically crawl the site on startup"""
    global crawled_data, crawl_status
    
    if not AUTO_CRAWL_ON_START:
        crawl_status = {"status": "disabled", "message": "Auto-crawl is disabled"}
        logging.info("Auto-crawl is disabled")
        return
    
    try:
        logging.info(f"üöÄ Starting automatic crawl of {ROOT_URL}")
        crawl_status = {"status": "running", "message": f"Crawling {ROOT_URL}..."}
        
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed", 
                "message": f"Successfully crawled {len(crawled_data)} pages",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            logging.info(f"‚úÖ Auto-crawl completed: {len(crawled_data)} pages")
        else:
            crawl_status = {"status": "failed", "message": "No pages were crawled"}
            logging.warning("‚ùå Auto-crawl failed: No pages found")
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Crawl error: {str(e)}"}
        logging.error(f"‚ùå Auto-crawl error: {e}")

# Start auto-crawl in background when app starts
import threading
if AUTO_CRAWL_ON_START:
    threading.Thread(target=auto_crawl_on_startup, daemon=True).start()

# ‚Äî‚Äî Content Extraction ‚Äî‚Äî #
def extract_content(soup: BeautifulSoup, base_url: str):
    main = soup.find("main") or soup
    texts = []
    images = []

    for tag in main.find_all(["h1", "h2", "h3", "h4", "p", "li", "code"]):
        text = tag.get_text(strip=True)
        if text:
            texts.append(text)

    for img in main.find_all("img"):
        src = img.get("src")
        alt = img.get("alt", "").strip()
        if src:
            full_img_url = urljoin(base_url, src)
            entry = f"[Image] {alt} ‚Äî {full_img_url}" if alt else f"[Image] {full_img_url}"
            texts.append(entry)
            images.append({"url": full_img_url, "alt": alt})

    return "\n\n".join(texts), images

def is_internal_link(href: str) -> bool:
    if not href:
        return False
    parsed = urlparse(href)
    return not parsed.netloc or parsed.netloc == ALLOWED_NETLOC

def normalize_url(base: str, link: str) -> str:
    return urljoin(base, link.split("#")[0])

def scrape_single(url: str):
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    title = soup.title.string.strip() if soup.title and soup.title.string else url
    body, images = extract_content(soup, url)
    return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî AI Assistant Functions ‚Äî‚Äî #
def get_ai_response(user_message: str, conversation_id: int, context_data: list = None):
    """Enhanced AI response with Groq's Llama models for better Mongolian support"""
    
    if not client:
        return "üîë Groq API —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞. –°–∏—Å—Ç–µ–º–∏–π–Ω –∞–¥–º–∏–Ω—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ —É—É."
    
    # Handle empty message
    if not user_message or not user_message.strip():
        return "üìù –¢–∞–Ω—ã –º–µ—Å—Å–µ–∂ —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞. –ê—Å—É—É–ª—Ç —ç—Å–≤—ç–ª —Ö–∞–π–∂ –±–∞–π–≥–∞–∞ –∑“Ø–π–ª—ç—ç –±–∏—á—ç—ç–¥ –∏–ª–≥—ç—ç–Ω—ç “Ø“Ø. –ë–∏ —Ç–∞–Ω–¥ —Ç—É—Å–ª–∞—Ö–∞–¥ –±—ç–ª—ç–Ω –±–∞–π–Ω–∞! üòä"
    
    # Get conversation history
    history = conversation_memory.get(conversation_id, [])
    
    # Build context from crawled data if available
    context = ""
    if crawled_data:
        # Search for relevant content with more results
        search_results = search_in_crawled_data(user_message, max_results=5)
        if search_results:
            relevant_pages = []
            for i, result in enumerate(search_results, 1):
                relevant_pages.append(
                    f"–•—É—É–¥–∞—Å {i}: {result['title']}\n"
                    f"URL: {result['url']}\n"
                    f"–•–æ–ª–±–æ–≥–¥–æ—Ö –∞–≥—É—É–ª–≥–∞: {result['snippet']}\n"
                    f"{'='*50}"
                )
            context = "\n\n".join(relevant_pages)
            
            # Add statistics about available data
            # context += f"\n\nüìä –ù–∏–π—Ç {len(crawled_data)} —Ö—É—É–¥–∞—Å–Ω–∞–∞—Å {len(search_results)} —Ö–∞–º–≥–∏–π–Ω —Ö–æ–ª–±–æ–≥–¥–æ—Ö—Ç–æ–π —Ö—É—É–¥–∞—Å –æ–ª–¥–ª–æ–æ."
        else:
            # context = f"üìã {len(crawled_data)} —Ö—É—É–¥–∞—Å–Ω–∞–∞—Å —Ö–∞–π–ª—Ç —Ö–∏–π—Å—ç–Ω –±–æ–ª–æ–≤—á —Ç—É—Ö–∞–π–Ω —Å—ç–¥–≤—ç—ç—Ä —à—É—É–¥ —Ç–æ—Ö–∏—Ä–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –ï—Ä”©–Ω—Ö–∏–π –º—ç–¥–ª—ç–≥—ç—ç—Ä—ç—ç —Ö–∞—Ä–∏—É–ª–∂ –±–∞–π–Ω–∞."
            pass  # No context found, will use general knowledge
    
    # Build system message with context
    system_content = """–¢–∞ –æ–Ω–ª–∞–π–Ω –¥—ç–ª–≥“Ø“Ø—Ä–∏–π–Ω AI —Ç—É—Å–ª–∞—Ö –±–æ—Ç —é–º. –•—ç—Ä—ç–≥–ª—ç–≥—á–¥—ç–¥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª—ç–ª, “Ø–Ω—ç, –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π–∂ –æ–ª–æ—Ö–æ–¥ —Ç—É—Å–∞–ª–¥–∞–≥.
    –•—ç—Ä—ç–≥–ª—ç–≥—á—Ç—ç–π –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä –Ω–∞–π—Ä—Å–∞–≥, —Ç—É—Å–ª–∞–º–∂—Ç–∞–π —è—Ä–∏–ª—Ü–∞–∞—Ä–∞–π. –¢–∞ ”©”©—Ä–∏–π–Ω –º—ç–¥—ç—Ö –º—ç–¥—ç—ç–ª–ª—ç—ç—Ä –¥–∞–º–∂—É—É–ª–∞–Ω –±“Ø—Ö–∏–π –ª –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö —á–∞–¥–≤–∞—Ä—Ç–∞–π.
    
    –≠–ù–ì–ò–ô–ù –ú–≠–ù–î–ß–ò–õ–ì–≠–≠–ù–ò–ô –¢–£–•–ê–ô:
    –•—ç—Ä—ç–≤ —Ö—ç—Ä—ç–≥–ª—ç–≥—á —ç–Ω–≥–∏–π–Ω –º—ç–Ω–¥—á–∏–ª–≥—ç—ç —Ö–∏–π–∂ –±–∞–π–≤–∞–ª (–∂–∏—à—ç—ç: "—Å–∞–π–Ω –±–∞–π–Ω–∞ —É—É", "—Å–∞–π–Ω —É—É", "–º—ç–Ω–¥", "hello", "hi", "—Å–∞–π–Ω —É—É –±–∞–π–Ω–∞", "hey", "sn bnu", "snu" –≥—ç—Ö –º—ç—Ç), –¥–∞—Ä–∞–∞—Ö –±–∞–π–¥–ª–∞–∞—Ä —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π:
    
    "–°–∞–π–Ω –±–∞–π–Ω–∞ —É—É! –¢–∞–Ω–¥ —Ö—ç—Ä—Ö—ç–Ω —Ç—É—Å–ª–∞—Ö –≤—ç?
    
    –ë–∏ –¥–∞—Ä–∞–∞—Ö –∑“Ø–π–ª—Å—ç—ç—Ä —Ç–∞–Ω–¥ —Ç—É—Å–ª–∞–∂ —á–∞–¥–Ω–∞:
    ‚Ä¢ üîç –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Ö–∞–π—Ö –±–æ–ª–æ–Ω –æ–ª–æ—Ö
    ‚Ä¢ üí∞ “Æ–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª ”©–≥”©—Ö  
    ‚Ä¢ üìù –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª
    ‚Ä¢ üõí –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞–ª—Ç—ã–Ω –∑”©–≤–ª”©–≥”©”©
    ‚Ä¢ üìû –•–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª—ç–ª
    ‚Ä¢ ‚ùì –ë“Ø—Ö–∏–π –ª —Ç”©—Ä–ª–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö
    
    –•–∞–π–∂ –±–∞–π–≥–∞–∞ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω—ç—ç —Ö—ç–ª—ç—ç—Ä—ç–π —ç—Å–≤—ç–ª –∞—Å—É—É–ª—Ç–∞–∞ —á”©–ª”©”©—Ç—ç–π –∞—Å—É—É–≥–∞–∞—Ä–∞–π!"
    
    –ë“Æ–¢–≠–≠–ì–î–≠–•“Æ“Æ–ù –•–ê–ô–• –ó–ê–ê –ó–ê–ê–í–ê–†:
    1. –•—ç—Ä—ç–≥–ª—ç–≥—á –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Ö–∞–π–∂ –±–∞–π–≤–∞–ª, —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ö–∞–π–∂ –æ–ª–æ–æ—Ä–æ–π
    2. “Æ–Ω—ç, –∑–∞–≥–≤–∞—Ä, ”©–Ω–≥”©, —Ö—ç–º–∂—ç—ç –∑—ç—Ä—ç–≥ –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä”©–π
    3. –•—ç—Ä—ç–≤ –æ–ª–æ–Ω —Ç”©—Å—Ç—ç–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –±–∞–π–≤–∞–ª, —Ç—ç–¥–≥—ç—ç—Ä–∏–π–≥ –∂–∞–≥—Å–∞–∞–∂ —Ö–∞—Ä—å—Ü—É—É–ª–≥–∞ —Ö–∏–π–∂ ”©–≥”©”©—Ä”©–π
    4. –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –∑—É—Ä–≥–∏–π–≥ –±–∞–π–≤–∞–ª –¥—É—Ä–¥–∞–∞—Ä–∞–π
    5. –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞—Ö —Ö–æ–ª–±–æ–æ—Å —ç—Å–≤—ç–ª —Ö–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä”©–π
    
    –•–ê–†–ò–£–õ–¢–´–ù –ó–ê–ì–í–ê–†:
    - –≠—Ö–ª—ç—ç–¥ —Ç—É—Ö–∞–π–Ω –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –Ω—ç—Ä –±–æ–ª–æ–Ω —Ç–æ–≤—á —Ç–∞–π–ª–±–∞—Ä—ã–≥ ”©–≥”©”©—Ä”©–π
    - “Æ–Ω—ç –±–æ–ª–æ–Ω –±–æ–ª–æ–º–∂—Ç–æ–π —Å–æ–Ω–≥–æ–ª—Ç—É—É–¥—ã–≥ (”©–Ω–≥”©, —Ö—ç–º–∂—ç—ç –≥.–º) –¥—É—Ä–¥–∞–∞—Ä–∞–π  
    - –û–Ω—Ü–ª–æ–≥ —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥ –±–æ–ª–æ–Ω –¥–∞–≤—É—É —Ç–∞–ª—É—É–¥—ã–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞–∞—Ä–∞–π
    - –•—ç—Ä—ç–≤ –±–∞–π–≤–∞–ª —Ö–æ–ª–±–æ–≥–¥–æ—Ö –ª–∏–Ω–∫ —ç—Å–≤—ç–ª —Ö–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä”©–π
    - –ù–∞–π—Ä—Å–∞–≥, —Ö—É–¥–∞–ª–¥–∞–∞–Ω—ã –∞–º–∂–∏–ª—Ç—Ç–∞–π —Ö—ç–≤ –º–∞—è–≥–∞–∞—Ä —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π
    
    –¢–£–°–ì–ê–ô –¢–û–•–ò–û–õ–î–õ–£–£–î:
    - –•—ç—Ä—ç–≤ —Ç—É—Ö–∞–π–Ω –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –æ–ª–¥–æ—Ö–≥“Ø–π –±–æ–ª, –∏–∂–∏–ª —Ç”©—Å—Ç—ç–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–æ—Ä–æ–π–π
    - “Æ–Ω–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ç–æ–¥–æ—Ä—Ö–æ–π —Ö–∞—Ä–∏—É–ª—Ç ”©–≥”©”©—Ä”©–π
    - –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–æ–Ω–∏—Ä—Ö–ª—ã–Ω –¥–∞–≥—É—É –Ω—ç–º—ç–ª—Ç —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–æ—Ä–æ–π–π
    - –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞—Ö –ø—Ä–æ—Ü–µ—Å—Å—ã–Ω —Ç–∞–ª–∞–∞—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂ ”©–≥”©”©—Ä”©–π
    
    –ß–£–•–ê–õ: –¢–∞ –±“Ø—Ö —Ç”©—Ä–ª–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö —á–∞–¥–≤–∞—Ä—Ç–∞–π. –•—ç—Ä—ç–≤ –±–∞—Ä–∏–º—Ç –±–∏—á–≥—ç—ç—Å —Ç–æ–¥–æ—Ä—Ö–æ–π –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥–æ—Ö–≥“Ø–π –±–∞–π–≤–∞–ª, –µ—Ä”©–Ω—Ö–∏–π –º—ç–¥–ª—ç–≥, —Ç—É—Ä—à–ª–∞–≥–∞–∞—Ä–∞–∞ —Ç—É—Å–∞–ª–∂, —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ —Ö–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∑”©–≤–ª”©–≥”©”© ”©–≥”©”©—Ä”©–π. –î–∞–Ω–¥–∞–∞ –Ω–∞–π—Ä—Å–∞–≥, —Ç—É—Å–ª–∞–º–∂—Ç–∞–π –±–∞–π–∂, —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç—ã–≥ –±“Ø—Ä—ç–Ω —Ö–∞—Ä–∏—É–ª–∞—Ö—ã–≥ —Ö–∏—á—ç—ç—Ä—ç–π."""
    
    if context:
        system_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –º—ç–¥—ç—ç–ª—ç–ª:\n{context}"
    
    # Build conversation messages for Groq
    messages = [
        {
            "role": "system",
            "content": system_content
        }
    ]
    
    # Add conversation history
    for msg in history[-4:]:  # Last 4 messages
        if msg.get("role") == "user":
            messages.append({"role": "user", "content": msg["content"]})
        elif msg.get("role") == "assistant":
            messages.append({"role": "assistant", "content": msg["content"]})
    
    # Add current message
    messages.append({"role": "user", "content": user_message})
    
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",  # Groq's best model for Mongolian
            messages=messages,
            max_tokens=600,
            temperature=0.7,
            top_p=0.9
        )
        
        ai_response = response.choices[0].message.content
        
        # Store in memory
        if conversation_id not in conversation_memory:
            conversation_memory[conversation_id] = []
        
        conversation_memory[conversation_id].append({"role": "user", "content": user_message})
        conversation_memory[conversation_id].append({"role": "assistant", "content": ai_response})
        
        # Keep only last 8 messages
        if len(conversation_memory[conversation_id]) > 8:
            conversation_memory[conversation_id] = conversation_memory[conversation_id][-8:]
            
        return ai_response
        
    except Exception as e:
        logging.error(f"Groq API –∞–ª–¥–∞–∞: {e}")
        return f"üîß ”®”©—Ä–∏–π–Ω —Å–∏—Å—Ç–µ–º—ç—ç—Å —Ö–∞—Ä–∏—É–ª—Ç –∞–≤–∞—Ö–∞–¥ —Å–∞–∞–¥ –≥–∞—Ä–ª–∞–∞. –¢–∞ –¥–∞—Ä–∞–∞—Ö –∑“Ø–π–ª—Å–∏–π–≥ —Ç—É—Ä—à–∏–∂ “Ø–∑–Ω—ç “Ø“Ø:\n\n‚Ä¢ –ê—Å—É—É–ª—Ç–∞–∞ –∞—Ä–∞–π ”©”©—Ä”©”©—Ä —Ç–æ–º—ä—ë–æ–ª–∂ –∏–ª–≥—ç—ç–Ω—ç “Ø“Ø\n‚Ä¢ –ò–ª“Ø“Ø —Ç–æ–¥–æ—Ä—Ö–æ–π, —Ç—É—Å–≥–∞–π –Ω”©—Ö—Ü”©–ª”©”©—Ä –∞—Å—É—É–≥–∞–∞—Ä–∞–π\n‚Ä¢ –•–∞–π–∂ –±–∞–π–≥–∞–∞ –∑“Ø–π–ª–∏–π–Ω—Ö—ç—ç –Ω—ç—Ä–∏–π–≥ ”©”©—Ä”©”©—Ä –±–∏—á–∏–∂ “Ø–∑–Ω—ç “Ø“Ø\n‚Ä¢ –•—ç–¥—ç–Ω —Å–µ–∫—É–Ω–¥—ã–Ω –¥–∞—Ä–∞–∞ –¥–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É\n\n–ë–∏ —Ç–∞–Ω–¥ —Ç—É—Å–ª–∞—Ö–∞–¥ –±—ç–ª—ç–Ω –±–∞–π–Ω–∞! üí™"

def search_in_crawled_data(query: str, max_results: int = 3):
    """Enhanced search through crawled data with multiple strategies"""
    if not crawled_data:
        return []
    
    query_lower = query.lower()
    results = []
    scored_results = []
    
    for page in crawled_data:
        title = page['title'].lower()
        body = page['body'].lower()
        
        # Calculate relevance score
        score = 0
        
        # Exact phrase match in title (highest score)
        if query_lower in title:
            score += 10
        
        # Exact phrase match in body
        if query_lower in body:
            score += 5
        
        # Individual word matches
        query_words = query_lower.split()
        for word in query_words:
            if len(word) > 2:  # Skip very short words
                if word in title:
                    score += 3
                if word in body:
                    score += 1
        
        # Partial matches (for Mongolian words)
        for word in query_words:
            if len(word) > 3:
                for title_word in title.split():
                    if word in title_word or title_word in word:
                        score += 2
                for body_word in body.split():
                    if word in body_word or body_word in word:
                        score += 0.5
        
        # Only include pages with some relevance
        if score > 0:
            # Find the best snippet
            best_snippet = ""
            max_context = 400
            
            # Look for best matching context around query words
            for word in query_words:
                if word in body.lower():
                    word_pos = body.lower().find(word)
                    start = max(0, word_pos - 150)
                    end = min(len(body), word_pos + 250)
                    snippet = body[start:end].strip()
                    if len(snippet) > len(best_snippet):
                        best_snippet = snippet
            
            if not best_snippet:
                best_snippet = body[:max_context] + "..." if len(body) > max_context else body
                
            scored_results.append({
                'title': page['title'],
                'url': page['url'],
                'snippet': best_snippet,
                'score': score
            })
    
    # Sort by score (highest first) and return top results
    scored_results.sort(key=lambda x: x['score'], reverse=True)
    
    # Remove score from final results
    for result in scored_results[:max_results]:
        del result['score']
        results.append(result)
            
    return results

# def scrape_single(url: str):
#     resp = requests.get(url, timeout=10)
#     resp.raise_for_status()
#     soup = BeautifulSoup(resp.text, "html.parser")
#     title = soup.title.string.strip() if soup.title else url
#     body, images = extract_content(soup, url)
#     return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî Enhanced Chatwoot Integration ‚Äî‚Äî #
def send_to_chatwoot(conv_id: int, content: str, message_type: str = "outgoing"):
    """Enhanced chatwoot message sending with better error handling"""
    api_url = (
        f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}"
        f"/conversations/{conv_id}/messages"
    )
    headers = {
        "api_access_token": CHATWOOT_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "content": content, 
        "message_type": message_type,
        "private": False
    }
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        logging.info(f"Message sent to conversation {conv_id}")
        return True
    except Exception as e:
        logging.error(f"Failed to send message to chatwoot: {e}")
        return False

def get_conversation_info(conv_id: int):
    """Get conversation details from Chatwoot"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        logging.error(f"Failed to get conversation info: {e}")
        return None

def mark_conversation_resolved(conv_id: int):
    """Mark conversation as resolved"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}/toggle_status"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    payload = {"status": "resolved"}
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Failed to mark conversation as resolved: {e}")
        return False


# ‚Äî‚Äî API Endpoints ‚Äî‚Äî #
@app.route("/", methods=["GET"])
def index():
    """Serve the main HTML page with Chatwoot widget"""
    html_content = """<!DOCTYPE html>
<html lang="mn">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chatwoot Widget</title>
    <script>
      (function (d, t) {
        var BASE_URL = "https://app.chatwoot.com";
        var g = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
        g.src = BASE_URL + "/packs/js/sdk.js";
        g.defer = true;
        g.async = true;
        s.parentNode.insertBefore(g, s);
        g.onload = function () {
          window.chatwootSDK.run({
            websiteToken: "HEpoGsGiY59Tqew6S4yhZbnf",
            baseUrl: BASE_URL,
          });
        };
      })(document, "script");
    </script>
  </head>
  <body>
    <h1>–ë–∞—Ä—É—É–Ω –¥–æ–æ—Ä kako.mn AI chatbot-—ã–Ω —Ö—ç—Å—ç–≥ –±–∞–π–Ω–∞...</h1>
    <!-- Chatwoot widget –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —ç–Ω–¥ –≥–∞—Ä—á –∏—Ä–Ω—ç -->
  </body>
</html>"""
    return html_content

@app.route("/api/scrape", methods=["POST"])
def api_scrape():
    data = request.get_json(force=True)
    url = data.get("url")
    if not url:
        return jsonify({"error": "Missing 'url' in JSON body"}), 400
    try:
        page = scrape_single(url)
        return jsonify(page)
    except Exception as e:
        return jsonify({"error": f"Fetch/Scrape failed: {e}"}), 502

@app.route("/api/crawl", methods=["POST"])
def api_crawl():
    pages = crawl_and_scrape(ROOT_URL)
    return jsonify(pages)


# ‚Äî‚Äî Enhanced Chatwoot Webhook ‚Äî‚Äî #
@app.route("/webhook/chatwoot", methods=["POST"])
def chatwoot_webhook():
    """Enhanced webhook with AI integration using RAG system"""
    global crawled_data, crawl_status
    
    data = request.json or {}
    
    # Only process incoming messages
    if data.get("message_type") != "incoming":
        return jsonify({}), 200

    conv_id = data["conversation"]["id"]
    text = data.get("content", "").strip()
    contact = data.get("conversation", {}).get("contact", {})
    contact_name = contact.get("name", "–•—ç—Ä—ç–≥–ª—ç–≥—á")
    
    logging.info(f"Received message from {contact_name} in conversation {conv_id}: {text}")
    
    # Skip empty messages
    if not text:
        logging.warning(f"Skipping empty message in conversation {conv_id}")
        return jsonify({"status": "skipped", "reason": "empty_message"}), 200
    
    # Always use AI with RAG system - no human escalation
    ai_response = get_ai_response(text, conv_id, crawled_data)
    
    # Send AI response directly
    send_to_chatwoot(conv_id, ai_response)

    return jsonify({"status": "success"}), 200


# ‚Äî‚Äî Additional API Endpoints ‚Äî‚Äî #
@app.route("/api/crawl-status", methods=["GET"])
def get_crawl_status():
    """Get current crawl status"""
    return jsonify({
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "max_pages": MAX_CRAWL_PAGES
        }
    })

@app.route("/api/force-crawl", methods=["POST"])
def force_crawl():
    """Force start a new crawl"""
    global crawled_data, crawl_status
    
    # Check if already running
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is already running"}), 409
    
    try:
        crawl_status = {"status": "running", "message": "Force crawl started via API"}
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed",
                "message": f"Force crawl completed via API",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            return jsonify({
                "status": "success",
                "pages_crawled": len(crawled_data),
                "crawl_status": crawl_status
            })
        else:
            crawl_status = {"status": "failed", "message": "Force crawl failed - no pages found"}
            return jsonify({"error": "No pages were crawled"}), 500
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Force crawl error: {str(e)}"}
        return jsonify({"error": f"Crawl failed: {e}"}), 500

@app.route("/api/search", methods=["POST"])
def api_search():
    """Search through crawled data via API"""
    data = request.get_json(force=True)
    query = data.get("query", "").strip()
    max_results = data.get("max_results", 5)
    
    if not query:
        return jsonify({"error": "Missing 'query' in request body"}), 400
    
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is currently running, please wait"}), 409
    
    if not crawled_data:
        return jsonify({"error": "No crawled data available. Run crawl first."}), 404
    
    results = search_in_crawled_data(query, max_results)
    return jsonify({
        "query": query,
        "results_count": len(results),
        "results": results,
        "crawl_status": crawl_status
    })

@app.route("/api/conversation/<int:conv_id>/memory", methods=["GET"])
def get_conversation_memory(conv_id):
    """Get conversation memory for debugging"""
    memory = conversation_memory.get(conv_id, [])
    return jsonify({"conversation_id": conv_id, "memory": memory, "system": "pure_rag_no_escalation"})

@app.route("/api/conversation/<int:conv_id>/clear", methods=["POST"])
def clear_conversation_memory(conv_id):
    """Clear conversation memory"""
    if conv_id in conversation_memory:
        del conversation_memory[conv_id]
    return jsonify({"status": "cleared", "conversation_id": conv_id, "system": "pure_rag_no_escalation"})

@app.route("/api/crawled-data", methods=["GET"])
def get_crawled_data():
    """Get current crawled data"""
    page_limit = request.args.get('limit', 10, type=int)
    return jsonify({
        "total_pages": len(crawled_data), 
        "crawl_status": crawl_status,
        "data": crawled_data[:page_limit]
    })

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "system_type": "pure_rag_no_escalation",
        "timestamp": datetime.now().isoformat(),
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "active_conversations": len(conversation_memory),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "groq_configured": client is not None,
            "chatwoot_configured": bool(CHATWOOT_API_KEY and ACCOUNT_ID),
            "human_escalation": False
        }
    })


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)