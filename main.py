import os
import time
import logging
import requests
from openai import OpenAI
import json
from urllib.parse import urljoin, urlparse
from flask import Flask, request, jsonify, render_template_string, send_from_directory
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, Optional

app = Flask(__name__, static_folder='.', static_url_path='/static')
logging.basicConfig(level=logging.INFO)

# ‚Äî‚Äî Config ‚Äî‚Äî #
ROOT_URL             = os.getenv("ROOT_URL", "https://kako.mn/")
DELAY_SEC            = float(os.getenv("DELAY_SEC", "0.5"))
ALLOWED_NETLOC       = urlparse(ROOT_URL).netloc
MAX_CRAWL_PAGES      = int(os.getenv("MAX_CRAWL_PAGES", "100"))
CHATWOOT_API_KEY     = os.getenv("CHATWOOT_API_KEY")
ACCOUNT_ID           = os.getenv("ACCOUNT_ID")
CHATWOOT_BASE_URL    = os.getenv("CHATWOOT_BASE_URL", "https://kako.mn/")
OPENAI_API_KEY       = os.getenv("OPENAI_API_KEY")
AUTO_CRAWL_ON_START  = os.getenv("AUTO_CRAWL_ON_START", "true").lower() == "true"

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ‚Äî‚Äî Memory Storage ‚Äî‚Äî #
conversation_memory = {}
crawled_data = []
crawl_status = {"status": "not_started", "message": "Crawling has not started yet"}

# ‚Äî‚Äî Crawl & Scrape ‚Äî‚Äî #
def crawl_and_scrape(start_url: str):
    visited = set()
    to_visit = {start_url}
    results = []

    while to_visit and len(visited) < MAX_CRAWL_PAGES:
        url = to_visit.pop()
        if url in visited:
            continue
        visited.add(url)

        try:
            logging.info(f"[Crawling] {url}")
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            logging.warning(f"Failed to fetch {url}: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.string.strip() if soup.title and soup.title.string else url
        body, images = extract_content(soup, url)

        results.append({
            "url": url,
            "title": title,
            "body": body,
            "images": images
        })

        for a in soup.find_all("a", href=True):
            href = a["href"]
            if is_internal_link(href):
                full = normalize_url(url, href)
                if full.startswith(ROOT_URL) and full not in visited:
                    to_visit.add(full)

        time.sleep(DELAY_SEC)

    return results

# ‚Äî‚Äî Startup Functions ‚Äî‚Äî #
def auto_crawl_on_startup():
    """Automatically crawl the site on startup"""
    global crawled_data, crawl_status
    
    if not AUTO_CRAWL_ON_START:
        crawl_status = {"status": "disabled", "message": "Auto-crawl is disabled"}
        logging.info("Auto-crawl is disabled")
        return
    
    try:
        logging.info(f"üöÄ Starting automatic crawl of {ROOT_URL}")
        crawl_status = {"status": "running", "message": f"Crawling {ROOT_URL}..."}
        
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed", 
                "message": f"Successfully crawled {len(crawled_data)} pages",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            logging.info(f"‚úÖ Auto-crawl completed: {len(crawled_data)} pages")
        else:
            crawl_status = {"status": "failed", "message": "No pages were crawled"}
            logging.warning("‚ùå Auto-crawl failed: No pages found")
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Crawl error: {str(e)}"}
        logging.error(f"‚ùå Auto-crawl error: {e}")

# Start auto-crawl in background when app starts
import threading
if AUTO_CRAWL_ON_START:
    threading.Thread(target=auto_crawl_on_startup, daemon=True).start()

# ‚Äî‚Äî Content Extraction ‚Äî‚Äî #
def extract_content(soup: BeautifulSoup, base_url: str):
    main = soup.find("main") or soup
    texts = []
    images = []

    for tag in main.find_all(["h1", "h2", "h3", "h4", "p", "li", "code"]):
        text = tag.get_text(strip=True)
        if text:
            texts.append(text)

    for img in main.find_all("img"):
        src = img.get("src")
        alt = img.get("alt", "").strip()
        if src:
            full_img_url = urljoin(base_url, src)
            entry = f"[Image] {alt} ‚Äî {full_img_url}" if alt else f"[Image] {full_img_url}"
            texts.append(entry)
            images.append({"url": full_img_url, "alt": alt})

    return "\n\n".join(texts), images

def is_internal_link(href: str) -> bool:
    if not href:
        return False
    parsed = urlparse(href)
    return not parsed.netloc or parsed.netloc == ALLOWED_NETLOC

def normalize_url(base: str, link: str) -> str:
    return urljoin(base, link.split("#")[0])

def scrape_single(url: str):
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    title = soup.title.string.strip() if soup.title and soup.title.string else url
    body, images = extract_content(soup, url)
    return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî AI Assistant Functions ‚Äî‚Äî #
def get_ai_response(user_message: str, conversation_id: int, context_data: list = None):
    """Enhanced AI response with better context awareness"""
    
    if not client:
        return "üîë OpenAI API —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞. –ê–¥–º–∏–Ω—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ —É—É."
    
    # Get conversation history
    history = conversation_memory.get(conversation_id, [])
    
    # Build context from crawled data if available
    context = ""
    if crawled_data:
        # Search for relevant content
        search_results = search_in_crawled_data(user_message, max_results=3)
        if search_results:
            relevant_pages = []
            for result in search_results:
                relevant_pages.append(
                    f"–•—É—É–¥–∞—Å: {result['title']}\n"
                    f"URL: {result['url']}\n"
                    f"–•–æ–ª–±–æ–≥–¥–æ—Ö –∞–≥—É—É–ª–≥–∞: {result['snippet']}\n"
                )
            context = "\n\n".join(relevant_pages)
    
    # Build system message with context
    system_content = """–¢–∞ –æ–Ω–ª–∞–π–Ω –¥—ç–ª–≥“Ø“Ø—Ä–∏–π–Ω AI —Ç—É—Å–ª–∞—Ö –±–æ—Ç —é–º. –•—ç—Ä—ç–≥–ª—ç–≥—á–¥—ç–¥ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª—ç–ª, “Ø–Ω—ç, –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π–∂ –æ–ª–æ—Ö–æ–¥ —Ç—É—Å–∞–ª–¥–∞–≥.
    –•—ç—Ä—ç–≥–ª—ç–≥—á—Ç—ç–π –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä –Ω–∞–π—Ä—Å–∞–≥, —Ç—É—Å–ª–∞–º–∂—Ç–∞–π —è—Ä–∏–ª—Ü–∞–∞—Ä–∞–π.
    
    –≠–ù–ì–ò–ô–ù –ú–≠–ù–î–ß–ò–õ–ì–≠–≠–ù–ò–ô –¢–£–•–ê–ô:
    –•—ç—Ä—ç–≤ —Ö—ç—Ä—ç–≥–ª—ç–≥—á —ç–Ω–≥–∏–π–Ω –º—ç–Ω–¥—á–∏–ª–≥—ç—ç —Ö–∏–π–∂ –±–∞–π–≤–∞–ª (–∂–∏—à—ç—ç: "—Å–∞–π–Ω –±–∞–π–Ω–∞ —É—É", "—Å–∞–π–Ω —É—É", "–º—ç–Ω–¥", "hello", "hi", "—Å–∞–π–Ω —É—É –±–∞–π–Ω–∞", "hey", "sn bnu", "snu" –≥—ç—Ö –º—ç—Ç), –¥–∞—Ä–∞–∞—Ö –±–∞–π–¥–ª–∞–∞—Ä —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π:
    
    "–°–∞–π–Ω –±–∞–π–Ω–∞ —É—É! üõçÔ∏è –ë–∏ —Ç–∞–Ω–∞–π –æ–Ω–ª–∞–π–Ω –¥—ç–ª–≥“Ø“Ø—Ä–∏–π–Ω AI —Ç—É—Å–ª–∞—Ö —é–º. –¢–∞–Ω–¥ —Ö—ç—Ä—Ö—ç–Ω —Ç—É—Å–ª–∞—Ö –≤—ç?
    
    –ë–∏ –¥–∞—Ä–∞–∞—Ö –∑“Ø–π–ª—Å—ç—ç—Ä —Ç–∞–Ω–¥ —Ç—É—Å–ª–∞–∂ —á–∞–¥–Ω–∞:
    ‚Ä¢ üîç –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Ö–∞–π—Ö –±–æ–ª–æ–Ω –æ–ª–æ—Ö
    ‚Ä¢ üí∞ “Æ–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª ”©–≥”©—Ö  
    ‚Ä¢ üìù –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª
    ‚Ä¢ üõí –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞–ª—Ç—ã–Ω –∑”©–≤–ª”©–≥”©”©
    ‚Ä¢ üìû –•–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª—ç–ª
    
    –•–∞–π–∂ –±–∞–π–≥–∞–∞ –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω—ç—ç —Ö—ç–ª—ç—ç—Ä—ç–π —ç—Å–≤—ç–ª –∞—Å—É—É–ª—Ç–∞–∞ —á”©–ª”©”©—Ç—ç–π –∞—Å—É—É–≥–∞–∞—Ä–∞–π!"
    
    –ë“Æ–¢–≠–≠–ì–î–≠–•“Æ“Æ–ù –•–ê–ô–• –ó–ê–ê –ó–ê–ê–í–ê–†:
    1. –•—ç—Ä—ç–≥–ª—ç–≥—á –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Ö–∞–π–∂ –±–∞–π–≤–∞–ª, —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ö–∞–π–∂ –æ–ª–æ–æ—Ä–æ–π
    2. “Æ–Ω—ç, –∑–∞–≥–≤–∞—Ä, ”©–Ω–≥”©, —Ö—ç–º–∂—ç—ç –∑—ç—Ä—ç–≥ –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä—ç–π
    3. –•—ç—Ä—ç–≤ –æ–ª–æ–Ω —Ç”©—Å—Ç—ç–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –±–∞–π–≤–∞–ª, —Ç—ç–¥–≥—ç—ç—Ä–∏–π–≥ –∂–∞–≥—Å–∞–∞–∂ —Ö–∞—Ä—å—Ü—É—É–ª–≥–∞ —Ö–∏–π–∂ ”©–≥”©”©—Ä—ç–π
    4. –ë“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –∑—É—Ä–≥–∏–π–≥ –±–∞–π–≤–∞–ª –¥—É—Ä–¥–∞–∞—Ä–∞–π
    5. –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞—Ö —Ö–æ–ª–±–æ–æ—Å —ç—Å–≤—ç–ª —Ö–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä—ç–π
    
    –•–ê–†–ò–£–õ–¢–´–ù –ó–ê–ì–í–ê–†:
    - –≠—Ö–ª—ç—ç–¥ —Ç—É—Ö–∞–π–Ω –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω–∏–π –Ω—ç—Ä –±–æ–ª–æ–Ω —Ç–æ–≤—á —Ç–∞–π–ª–±–∞—Ä—ã–≥ ”©–≥”©”©—Ä—ç–π
    - “Æ–Ω—ç –±–æ–ª–æ–Ω –±–æ–ª–æ–º–∂—Ç–æ–π —Å–æ–Ω–≥–æ–ª—Ç—É—É–¥—ã–≥ (”©–Ω–≥”©, —Ö—ç–º–∂—ç—ç –≥.–º) –¥—É—Ä–¥–∞–∞—Ä–∞–π  
    - –û–Ω—Ü–ª–æ–≥ —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥ –±–æ–ª–æ–Ω –¥–∞–≤—É—É —Ç–∞–ª—É—É–¥—ã–≥ —Ç–∞–π–ª–±–∞—Ä–ª–∞–∞—Ä–∞–π
    - –•—ç—Ä—ç–≤ –±–∞–π–≤–∞–ª —Ö–æ–ª–±–æ–≥–¥–æ—Ö –ª–∏–Ω–∫ —ç—Å–≤—ç–ª —Ö–æ–ª–±–æ–æ –±–∞—Ä–∏—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ ”©–≥”©”©—Ä—ç–π
    - –ù–∞–π—Ä—Å–∞–≥, —Ö—É–¥–∞–ª–¥–∞–∞–Ω—ã –∞–º–∂–∏–ª—Ç—Ç–∞–π —Ö—ç–≤ –º–∞—è–≥–∞–∞—Ä —Ö–∞—Ä–∏—É–ª–∞–∞—Ä–∞–π
    
    –¢–£–°–ì–ê–ô –¢–û–•–ò–û–õ–î–õ–£–£–î:
    - –•—ç—Ä—ç–≤ —Ç—É—Ö–∞–π–Ω –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω –æ–ª–¥–æ—Ö–≥“Ø–π –±–æ–ª, –∏–∂–∏–ª —Ç”©—Å—Ç—ç–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–æ—Ä–æ–π–π
    - “Æ–Ω–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ç–æ–¥–æ—Ä—Ö–æ–π —Ö–∞—Ä–∏—É–ª—Ç ”©–≥”©”©—Ä”©–π
    - –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–æ–Ω–∏—Ä—Ö–ª—ã–Ω –¥–∞–≥—É—É –Ω—ç–º—ç–ª—Ç —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–æ—Ä–æ–π–π
    - –•—É–¥–∞–ª–¥–∞–Ω –∞–≤–∞—Ö –ø—Ä–æ—Ü–µ—Å—Å—ã–Ω —Ç–∞–ª–∞–∞—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞–∂ ”©–≥”©”©—Ä”©–π"""
    
    if context:
        system_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –º—ç–¥—ç—ç–ª—ç–ª:\n{context}"
    
    # Build conversation context
    messages = [
        {
            "role": "system", 
            "content": system_content
        }
    ]
    
    # Add conversation history
    for msg in history[-4:]:  # Last 4 messages
        messages.append(msg)
    
    # Add current message
    messages.append({"role": "user", "content": user_message})
    
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            max_tokens=500,  # Increased token limit for better responses
            temperature=0.7
        )
        
        ai_response = response.choices[0].message.content
        
        # Store in memory
        if conversation_id not in conversation_memory:
            conversation_memory[conversation_id] = []
        
        conversation_memory[conversation_id].append({"role": "user", "content": user_message})
        conversation_memory[conversation_id].append({"role": "assistant", "content": ai_response})
        
        # Keep only last 8 messages
        if len(conversation_memory[conversation_id]) > 8:
            conversation_memory[conversation_id] = conversation_memory[conversation_id][-8:]
            
        return ai_response
        
    except Exception as e:
        logging.error(f"OpenAI API –∞–ª–¥–∞–∞: {e}")
        return f"üîß AI-—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö–æ–¥ —Å–∞–∞–¥ –≥–∞—Ä–ª–∞–∞. –î–∞—Ä–∞–∞—Ö –∑“Ø–π–ª—Å–∏–π–≥ —Ç—É—Ä—à–∏–∂ “Ø–∑–Ω—ç “Ø“Ø:\n‚Ä¢ –ê—Å—É—É–ª—Ç–∞–∞ –¥–∞—Ö–∏–Ω –∏–ª–≥—ç—ç–Ω—ç “Ø“Ø\n‚Ä¢ –ê—Å—É—É–ª—Ç–∞–∞ —Ç–æ–¥–æ—Ä—Ö–æ–π –±–æ–ª–≥–æ–Ω–æ —É—É\n‚Ä¢ –•–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ö–∞–π–∂ “Ø–∑–Ω—ç “Ø“Ø\n\n–ê–ª–¥–∞–∞–Ω—ã –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π: {str(e)[:100]}"

def search_in_crawled_data(query: str, max_results: int = 3):
    """Simple search through crawled data"""
    if not crawled_data:
        return []
    
    query_lower = query.lower()
    results = []
    
    for page in crawled_data:
        title = page['title'].lower()
        body = page['body'].lower()
        
        # Check if query matches in title or body
        if (query_lower in title or 
            query_lower in body or 
            any(word in title or word in body for word in query_lower.split())):
            
            # Find the most relevant snippet
            query_words = query_lower.split()
            best_snippet = ""
            max_context = 300
            
            for word in query_words:
                if word in body.lower():
                    start = max(0, body.lower().find(word) - 100)
                    end = min(len(body), body.lower().find(word) + 200)
                    snippet = body[start:end]
                    if len(snippet) > len(best_snippet):
                        best_snippet = snippet
            
            if not best_snippet:
                best_snippet = body[:max_context] + "..." if len(body) > max_context else body
                
            results.append({
                'title': page['title'],
                'url': page['url'],
                'snippet': best_snippet
            })
            
            # Stop when we have enough results
            if len(results) >= max_results:
                break
            
    return results

# def scrape_single(url: str):
#     resp = requests.get(url, timeout=10)
#     resp.raise_for_status()
#     soup = BeautifulSoup(resp.text, "html.parser")
#     title = soup.title.string.strip() if soup.title else url
#     body, images = extract_content(soup, url)
#     return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî Enhanced Chatwoot Integration ‚Äî‚Äî #
def send_to_chatwoot(conv_id: int, content: str, message_type: str = "outgoing"):
    """Enhanced chatwoot message sending with better error handling"""
    api_url = (
        f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}"
        f"/conversations/{conv_id}/messages"
    )
    headers = {
        "api_access_token": CHATWOOT_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "content": content, 
        "message_type": message_type,
        "private": False
    }
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        logging.info(f"Message sent to conversation {conv_id}")
        return True
    except Exception as e:
        logging.error(f"Failed to send message to chatwoot: {e}")
        return False

def get_conversation_info(conv_id: int):
    """Get conversation details from Chatwoot"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        logging.error(f"Failed to get conversation info: {e}")
        return None

def mark_conversation_resolved(conv_id: int):
    """Mark conversation as resolved"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}/toggle_status"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    payload = {"status": "resolved"}
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Failed to mark conversation as resolved: {e}")
        return False


# ‚Äî‚Äî API Endpoints ‚Äî‚Äî #
@app.route("/", methods=["GET"])
def index():
    """Serve the main HTML page with Chatwoot widget"""
    html_content = """<!DOCTYPE html>
<html lang="mn">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chatwoot Widget</title>
    <script>
      (function (d, t) {
        var BASE_URL = "https://app.chatwoot.com";
        var g = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
        g.src = BASE_URL + "/packs/js/sdk.js";
        g.defer = true;
        g.async = true;
        s.parentNode.insertBefore(g, s);
        g.onload = function () {
          window.chatwootSDK.run({
            websiteToken: "HEpoGsGiY59Tqew6S4yhZbnf",
            baseUrl: BASE_URL,
          });
        };
      })(document, "script");
    </script>
  </head>
  <body>
    <h1>–ë–∞—Ä—É—É–Ω –¥–æ–æ—Ä kako.mn AI chatbot-—ã–Ω —Ö—ç—Å—ç–≥ –±–∞–π–Ω–∞...</h1>
    <!-- Chatwoot widget –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —ç–Ω–¥ –≥–∞—Ä—á –∏—Ä–Ω—ç -->
  </body>
</html>"""
    return html_content

@app.route("/api/scrape", methods=["POST"])
def api_scrape():
    data = request.get_json(force=True)
    url = data.get("url")
    if not url:
        return jsonify({"error": "Missing 'url' in JSON body"}), 400
    try:
        page = scrape_single(url)
        return jsonify(page)
    except Exception as e:
        return jsonify({"error": f"Fetch/Scrape failed: {e}"}), 502

@app.route("/api/crawl", methods=["POST"])
def api_crawl():
    pages = crawl_and_scrape(ROOT_URL)
    return jsonify(pages)


# ‚Äî‚Äî Enhanced Chatwoot Webhook ‚Äî‚Äî #
@app.route("/webhook/chatwoot", methods=["POST"])
def chatwoot_webhook():
    """Enhanced webhook with AI integration"""
    global crawled_data, crawl_status
    
    data = request.json or {}
    
    # Only process incoming messages
    if data.get("message_type") != "incoming":
        return jsonify({}), 200

    conv_id = data["conversation"]["id"]
    text = data.get("content", "").strip()
    contact = data.get("conversation", {}).get("contact", {})
    contact_name = contact.get("name", "–•—ç—Ä—ç–≥–ª—ç–≥—á")
    
    logging.info(f"Received message from {contact_name} in conversation {conv_id}: {text}")
    
    # Get conversation history
    history = conversation_memory.get(conv_id, [])
    
    # Try to answer with AI first
    ai_response = get_ai_response(text, conv_id, crawled_data)
    
    # Check if AI couldn't find good answer by searching crawled data
    search_results = search_in_crawled_data(text, max_results=3)
    
    # Check if this user was previously escalated but asking a new question
    was_previously_escalated = any(
        msg.get("role") == "system" and "escalated_to_human" in msg.get("content", "")
        for msg in history
    )
    
    # Let AI evaluate its own response quality and decide if human help is needed
    needs_human_help = should_escalate_to_human(text, search_results, ai_response, history)
    
    # If user was previously escalated but AI can answer this new question, respond with AI
    if was_previously_escalated and not needs_human_help:
        # AI can handle this new question even though user was escalated before
        response_with_note = f"{ai_response}\n\nüí° –•—ç—Ä—ç–≤ —ç–Ω—ç —Ö–∞—Ä–∏—É–ª—Ç —Ö–∞–Ω–≥–∞–ª—Ç–≥“Ø–π –±–æ–ª, –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ —É—É."
        send_to_chatwoot(conv_id, response_with_note)
        return jsonify({"status": "success"}), 200
    
    if needs_human_help:
        # Mark this conversation as escalated
        if conv_id not in conversation_memory:
            conversation_memory[conv_id] = []
        conversation_memory[conv_id].append({
            "role": "system", 
            "content": "escalated_to_human"
        })
        
        # AI thinks it can't handle this properly, escalate to human
        escalation_response = """ü§ù –ë–∏ —Ç–∞–Ω—ã –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —Ö–∞—Ä–∏—É–ª—Ç ”©–≥—á —á–∞–¥–∞—Ö–≥“Ø–π –±–∞–π–Ω–∞. –î—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥–∏–π–Ω —Ç—É—Å–ª–∞–º–∂ –∞–≤–∞—Ö—ã–≥ —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–∂ –±–∞–π–Ω–∞.

–¢—É—Å–ª–∞–º–∂–∏–π–Ω –±–∞–≥ —É–¥–∞—Ö–≥“Ø–π —Ç–∞–Ω–¥ —Ö–∞—Ä–∏—É–ª—Ç ”©–≥”©—Ö –±–æ–ª–Ω–æ."""
        
        send_to_chatwoot(conv_id, escalation_response)
    else:
        # AI is confident in its response, send it
        send_to_chatwoot(conv_id, ai_response)

    return jsonify({"status": "success"}), 200


def should_escalate_to_human(user_message: str, search_results: list, ai_response: str, history: list) -> bool:
    """AI evaluates its own response and decides if human help is needed"""
    
    # Use AI to evaluate its own response quality
    if not client:
        # Fallback without AI evaluation - be more lenient
        return len(user_message) > 50 and (not search_results or len(search_results) == 0)
    
    # Build context for AI self-evaluation
    context = f"""–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç: "{user_message}"

–ú–∞–Ω–∞–π –±–∞—Ä–∏–º—Ç –±–∏—á–≥—ç—ç—Å —Ö–∞–π—Å–∞–Ω “Ø—Ä –¥“Ø–Ω:
{f"–û–ª–¥—Å–æ–Ω: {len(search_results)} “Ø—Ä –¥“Ø–Ω" if search_results else "–ú—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π"}

–ú–∏–Ω–∏–π ”©–≥—Å”©–Ω —Ö–∞—Ä–∏—É–ª—Ç: "{ai_response}"

–Ø—Ä–∏–ª—Ü–ª–∞–≥—ã–Ω —Å“Ø“Ø–ª–∏–π–Ω –º–µ—Å—Å–µ–∂“Ø“Ø–¥:"""
    
    if history:
        recent_messages = [msg.get("content", "")[:100] for msg in history[-3:] if msg.get("role") == "user"]
        if recent_messages:
            context += "\n" + "\n".join(recent_messages)
    
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": """–¢–∞ ”©”©—Ä–∏–π–Ω ”©–≥—Å”©–Ω —Ö–∞—Ä–∏—É–ª—Ç—ã–≥ “Ø–Ω—ç–ª–∂, —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —ç—Å—ç—Ö–∏–π–≥ —à–∏–π–¥–Ω—ç.

–î–∞—Ä–∞–∞—Ö —Ç–æ—Ö–∏–æ–ª–¥–ª—É—É–¥–∞–¥ –ª —Ö“Ø–Ω–∏–π –∞–∂–∏–ª—Ç–Ω—ã —Ç—É—Å–ª–∞–º–∂ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π:
- –•—ç—Ä—ç–≥–ª—ç–≥—á —Ç–µ—Ö–Ω–∏–∫–∏–π–Ω –∞–ª–¥–∞–∞, —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω—ã –∞—Å—É—É–¥–ª–∞–∞—Ä —Ç—É—Å–ª–∞–º–∂ —Ö“Ø—Å—ç–∂ –±–∞–π–≥–∞–∞
- –ê–∫–∞—É–Ω—Ç, —Ç”©–ª–±”©—Ä, —Ö–æ—Å—Ç–∏–Ω–≥, –¥–æ–º—ç–π–Ω –∑—ç—Ä—ç–≥ Cloud.mn-–∏–π–Ω “Ø–π–ª—á–∏–ª–≥—ç—ç—Ç—ç–π —Ö–æ–ª–±–æ–æ—Ç–æ–π –∞—Å—É—É–¥–∞–ª
- –¢—É—Å–≥–∞–π —Ö“Ø—Å—ç–ª—Ç, –≥–æ–º–¥–æ–ª, —à—É—É—Ä—Ö–∞–π —Ç—É—Å–ª–∞–º–∂ —Ö—ç—Ä—ç–≥—Ç—ç–π –∞—Å—É—É–¥–∞–ª
- –•—ç—Ä—ç–≥–ª—ç–≥—á ”©”©—Ä”©”© "–∞–∂–∏–ª—Ç–Ω—ã–≥ —Ö“Ø—Å—ç–∂ –±–∞–π–Ω–∞" –≥—ç–∂ —Ç–æ–¥–æ—Ä—Ö–æ–π —Ö—ç–ª—Å—ç–Ω —Ç–æ—Ö–∏–æ–ª–¥–æ–ª
- –ú–∏–Ω–∏–π —Ö–∞—Ä–∏—É–ª—Ç –Ω—å —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç—ã–Ω “Ø–Ω–¥—Å—ç–Ω —Å—ç–¥–≤—ç—ç—Å –æ–≥—Ç —Ö–æ–ª–¥—Å–æ–Ω –±–æ–ª

–î–∞—Ä–∞–∞—Ö —Ç–æ—Ö–∏–æ–ª–¥–ª—É—É–¥–∞–¥ —Ö“Ø–Ω–∏–π —Ç—É—Å–ª–∞–º–∂ –®–ê–ê–†–î–õ–ê–ì–ê–ì“Æ–ô:
- –≠–Ω–≥–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª –∞—Å—É—É—Ö (Cloud.mn docs-–∏–π–Ω —Ç—É—Ö–∞–π)
- –ï—Ä”©–Ω—Ö–∏–π –∑”©–≤–ª”©–≥”©”© –∞–≤–∞—Ö
- –¢–µ—Ö–Ω–∏–∫–∏–π–Ω –º—ç–¥–ª—ç–≥ —Å—É–¥–ª–∞—Ö
- –ë–∏ —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π —Ö–∞—Ä–∏—É–ª—Ç ”©–≥—á —á–∞–¥—Å–∞–Ω —Ç–æ—Ö–∏–æ–ª–¥–æ–ª
- –•—ç—Ä—ç–≥–ª—ç–≥—á –∑“Ø–≥—ç—ç—Ä –ª –º—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π–∂ –±–∞–π–≥–∞–∞

”®”©—Ä–∏–π–Ω—Ö”©”© —Ö–∞—Ä–∏—É–ª—Ç–∞–Ω–¥ –∏—Ç–≥—ç–ª—Ç—ç–π –±–∞–π–∂, —Ö—ç—Ä—ç–≥–ª—ç–≥—á –¥–∞—Ö–∏–Ω –∞—Å—É—É–∂ –±–æ–ª–Ω–æ –≥—ç–¥–≥–∏–π–≥ —Å–∞–Ω–∞–∞—Ä–∞–π.

–•–∞—Ä–∏—É–ª—Ç–∞–∞ –∑”©–≤—Ö”©–Ω 'YES' (—Ö“Ø–Ω–∏–π —Ç—É—Å–ª–∞–º–∂ —Ö—ç—Ä—ç–≥—Ç—ç–π) —ç—Å–≤—ç–ª 'NO' (–º–∏–Ω–∏–π —Ö–∞—Ä–∏—É–ª—Ç —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π) –≥—ç–∂ ”©–≥–Ω”© “Ø“Ø."""
                },
                {
                    "role": "user", 
                    "content": context
                }
            ],
            max_tokens=10,
            temperature=0.2
        )
        
        ai_decision = response.choices[0].message.content.strip().upper()
        logging.info(f"AI self-evaluation for '{user_message[:30]}...': {ai_decision}")
        return ai_decision == "YES"
        
    except Exception as e:
        logging.error(f"AI self-evaluation error: {e}")
        # More lenient fallback - don't escalate by default
        return False


# ‚Äî‚Äî Additional API Endpoints ‚Äî‚Äî #
@app.route("/api/crawl-status", methods=["GET"])
def get_crawl_status():
    """Get current crawl status"""
    return jsonify({
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "max_pages": MAX_CRAWL_PAGES
        }
    })

@app.route("/api/force-crawl", methods=["POST"])
def force_crawl():
    """Force start a new crawl"""
    global crawled_data, crawl_status
    
    # Check if already running
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is already running"}), 409
    
    try:
        crawl_status = {"status": "running", "message": "Force crawl started via API"}
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed",
                "message": f"Force crawl completed via API",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            return jsonify({
                "status": "success",
                "pages_crawled": len(crawled_data),
                "crawl_status": crawl_status
            })
        else:
            crawl_status = {"status": "failed", "message": "Force crawl failed - no pages found"}
            return jsonify({"error": "No pages were crawled"}), 500
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Force crawl error: {str(e)}"}
        return jsonify({"error": f"Crawl failed: {e}"}), 500

@app.route("/api/search", methods=["POST"])
def api_search():
    """Search through crawled data via API"""
    data = request.get_json(force=True)
    query = data.get("query", "").strip()
    max_results = data.get("max_results", 5)
    
    if not query:
        return jsonify({"error": "Missing 'query' in request body"}), 400
    
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is currently running, please wait"}), 409
    
    if not crawled_data:
        return jsonify({"error": "No crawled data available. Run crawl first."}), 404
    
    results = search_in_crawled_data(query, max_results)
    return jsonify({
        "query": query,
        "results_count": len(results),
        "results": results,
        "crawl_status": crawl_status
    })

@app.route("/api/conversation/<int:conv_id>/memory", methods=["GET"])
def get_conversation_memory(conv_id):
    """Get conversation memory for debugging"""
    memory = conversation_memory.get(conv_id, [])
    return jsonify({"conversation_id": conv_id, "memory": memory})

@app.route("/api/conversation/<int:conv_id>/clear", methods=["POST"])
def clear_conversation_memory(conv_id):
    """Clear conversation memory"""
    if conv_id in conversation_memory:
        del conversation_memory[conv_id]
    return jsonify({"status": "cleared", "conversation_id": conv_id})

@app.route("/api/crawled-data", methods=["GET"])
def get_crawled_data():
    """Get current crawled data"""
    page_limit = request.args.get('limit', 10, type=int)
    return jsonify({
        "total_pages": len(crawled_data), 
        "crawl_status": crawl_status,
        "data": crawled_data[:page_limit]
    })

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "active_conversations": len(conversation_memory),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "openai_configured": client is not None,
            "chatwoot_configured": bool(CHATWOOT_API_KEY and ACCOUNT_ID)
        }
    })


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)